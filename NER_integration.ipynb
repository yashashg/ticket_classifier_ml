{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "685f6f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "df = pd.read_csv(\"ai_dev_assignment_tickets_complex_1000.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a10f37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample product list: ['smartwatch v2', 'ultraclean vacuum', 'soundwave 300', 'photosnap cam', 'vision led tv', 'ecobreeze ac', 'robochef blender', 'fitrun treadmill', 'powermax battery', 'protab x1']\n"
     ]
    }
   ],
   "source": [
    "# Get all unique product names (drop NaN just in case)\n",
    "product_list = df['product'].dropna().unique().tolist()\n",
    "product_list = [p.lower() for p in product_list]  # normalize\n",
    "print(\"Sample product list:\", product_list[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "999e1cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLP function to extract entities from text\n",
    "\n",
    "\n",
    "def extract_entities(text):\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    # --- Extract Product ---\n",
    "\n",
    "\n",
    "\n",
    "    extracted_products = set()\n",
    "    text_words = set(re.findall(r'\\w+', text_lower))  # tokenize text\n",
    "\n",
    "    for product in product_list:\n",
    "        product_words = set(product.lower().split())\n",
    "        if product_words & text_words:  # if any word matches\n",
    "            extracted_products.add(product)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # --- Extract Dates ---\n",
    "\n",
    "    date_entities = []\n",
    "    today = datetime.today()\n",
    "    if 'yesterday' in text_lower:\n",
    "        date_entities.append((today - timedelta(days=1)).strftime('%Y-%m-%d'))\n",
    "    if 'tomorrow' in text_lower:\n",
    "        date_entities.append((today + timedelta(days=1)).strftime('%Y-%m-%d'))\n",
    "    if 'today' in text_lower:\n",
    "        date_entities.append((today).strftime('%Y-%m-%d'))\n",
    "    \n",
    "    absolute_date_patterns = [\n",
    "        r'\\b\\d{4}[/-]\\d{1,2}[/-]\\d{1,2}\\b',   # YYYY/MM/DD or YYYY-MM-DD\n",
    "        r'\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{4}\\b'    # DD/MM/YYYY or DD-MM-YYYY\n",
    "    ]\n",
    "    for pattern in absolute_date_patterns:\n",
    "        for match in re.findall(pattern, text):\n",
    "            try:\n",
    "                # Try different formats\n",
    "                for fmt in ['%Y/%m/%d', '%Y-%m-%d', '%d/%m/%Y', '%d-%m-%Y']:\n",
    "                    try:\n",
    "                        parsed = datetime.strptime(match, fmt)\n",
    "                        date_entities.append(parsed.strftime('%Y-%m-%d'))\n",
    "                        break\n",
    "                    except:\n",
    "                        continue\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    # Handle textual dates: 3rd March, 5th Oct, 12 June, etc.\n",
    "    month_names = r'jan|feb|mar|apr|may|jun|jul|aug|sep|sept|oct|nov|dec|january|february|march|april|june|july|august|september|october|november|december'\n",
    "    text_date_pattern = rf'\\b\\d{{1,2}}(?:st|nd|rd|th)?\\s+({month_names})\\b'\n",
    "    for match in re.findall(text_date_pattern, text_lower, flags=re.IGNORECASE):\n",
    "        for m in re.finditer(rf'(\\d{{1,2}}(?:st|nd|rd|th)?)\\s+{match}', text_lower):\n",
    "            day = re.sub(r'(st|nd|rd|th)', '', m.group(1))\n",
    "            try:\n",
    "                parsed = datetime.strptime(f'{day} {match}', '%d %b').replace(year=today.year)\n",
    "            except:\n",
    "                try:\n",
    "                    parsed = datetime.strptime(f'{day} {match}', '%d %B').replace(year=today.year)\n",
    "                except:\n",
    "                    continue\n",
    "            date_entities.append(parsed.strftime('%Y-%m-%d'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # --- Extract Issue keywords ---\n",
    "    issue_keywords = [\n",
    "        'broken', 'not working', 'doesn’t work', 'stopped working', 'damaged',\n",
    "        'cracked', 'won’t start', 'flickering', 'slow', 'heating', 'noisy',\n",
    "        'defective', 'malfunction', 'problem', 'error', 'issue', 'freeze'\n",
    "    ]\n",
    "    found_issues = []\n",
    "    for keyword in issue_keywords:\n",
    "        if keyword in text_lower:\n",
    "            found_issues.append(keyword)\n",
    "\n",
    "    return {\n",
    "        'products': list(extracted_products),\n",
    "        'dates': date_entities,\n",
    "        'issues': found_issues\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "895ef931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'products': ['smartwatch v2'],\n",
       " 'dates': ['2025-10-05'],\n",
       " 'issues': ['broken', 'slow']}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_entities(\"Product  v2 is broken and I need it fixed by 5th october. It's been slow lately.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4f88b0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yasha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yasha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yasha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import joblib\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# === Download NLTK Resources ===\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4e616af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved models for urgency classification and SBERT\n",
    "clf = joblib.load('./models/xgboost_urgency_classifier.pkl')\n",
    "sbert_model = joblib.load('./models/sbert_model.pkl')\n",
    "# === Load Saved model for issue ===\n",
    "model = joblib.load(\"./models/issue_model.pkl\")\n",
    "tfidf = joblib.load(\"./models/issue_vectorizer.pkl\")\n",
    "encoder = joblib.load(\"./models/issue_type_encoder.pkl\")\n",
    "scaler = joblib.load(\"./models/issue_scaler.pkl\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "925c2c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## urgency prediction function\n",
    "\n",
    "def urgency_score(text):\n",
    "    keywords = ['urgent', 'asap', 'immediately', 'not working', 'waiting', 'help', 'escalated', 'delay', 'issue', 'problem']\n",
    "    text = text.lower()\n",
    "    return sum(kw in text for kw in keywords)\n",
    "\n",
    "# run the prediction on a new text\n",
    "def predict_urgency(text):\n",
    "    score = urgency_score(text)\n",
    "    if score >= 3:\n",
    "        return \"High\"\n",
    "    else:\n",
    "        emb = sbert_model.encode([text])\n",
    "        features = np.hstack([emb, np.array([[score]])])\n",
    "        pred = clf.predict(features)[0]\n",
    "        return \"High\" if pred == 1 else \"Not High\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e96e1742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# issue_type_predictor\n",
    "\n",
    "\n",
    "\n",
    "# === Define Text Preprocessing Functions ===\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = nltk.word_tokenize(clean_text(text))\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def get_sentiment(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "# === Prediction Function ===\n",
    "def predict_issue_type(ticket_text):\n",
    "    processed = preprocess_text(ticket_text)\n",
    "    length = len(ticket_text)\n",
    "    sentiment = get_sentiment(ticket_text)\n",
    "\n",
    "    X_tfidf = tfidf.transform([processed])\n",
    "    X_num = scaler.transform([[length, sentiment]])\n",
    "    X_final = hstack([X_tfidf, X_num])\n",
    "\n",
    "    pred = model.predict(X_final)\n",
    "    label = encoder.inverse_transform(pred)[0]\n",
    "    return label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a08a56bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_ticket(text):\n",
    "    # --- Issue Type Prediction ---\n",
    "    issue_pred = predict_issue_type(text)\n",
    "\n",
    "    # --- Urgency Prediction ---\n",
    "    urgency_pred = predict_urgency(text)\n",
    "\n",
    "    # --- Entity Extraction ---\n",
    "    entities = extract_entities(text)\n",
    "\n",
    "    return {\n",
    "        \"issue_type\": issue_pred,\n",
    "        \"urgency_level\": urgency_pred,\n",
    "        \"entities\": entities\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c43c2b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yasha\\Desktop\\New folder\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'issue_type': 'Product Defect',\n",
       " 'urgency_level': 'High',\n",
       " 'entities': {'products': ['smartwatch v2'],\n",
       "  'dates': ['2025-06-16'],\n",
       "  'issues': ['not working', 'issue']}}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_ticket(\"Today The smartwatch is not working and we need immediate help with this issue.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14788de9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
